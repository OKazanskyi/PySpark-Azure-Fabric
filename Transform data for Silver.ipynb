{"cells":[{"cell_type":"code","source":[" from pyspark.sql.types import *\n","    \n"," # Create the schema for the table\n"," orderSchema = StructType([\n","     StructField(\"SalesOrderNumber\", StringType()),\n","     StructField(\"SalesOrderLineNumber\", IntegerType()),\n","     StructField(\"OrderDate\", DateType()),\n","     StructField(\"CustomerName\", StringType()),\n","     StructField(\"Email\", StringType()),\n","     StructField(\"Item\", StringType()),\n","     StructField(\"Quantity\", IntegerType()),\n","     StructField(\"UnitPrice\", FloatType()),\n","     StructField(\"Tax\", FloatType())\n","     ])\n","    \n"," # Import all files from bronze folder of lakehouse\n"," df = spark.read.format(\"csv\").option(\"header\", \"true\").schema(orderSchema).load(\"Files/bronze/*.csv\")\n","    \n"," # Display the first 10 rows of the dataframe to preview your data\n"," display(df.head(10))"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"11bbb665-bb13-40fa-823b-4daa08e24ec4","statement_id":20,"state":"finished","livy_statement_state":"available","queued_time":"2024-04-06T20:03:01.0423235Z","session_start_time":null,"execution_start_time":"2024-04-06T20:03:01.4041832Z","execution_finish_time":"2024-04-06T20:03:04.8809095Z","parent_msg_id":"5017506f-f074-4ce1-9931-c51952df6aee"},"text/plain":"StatementMeta(, 11bbb665-bb13-40fa-823b-4daa08e24ec4, 20, Finished, Available)"},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.synapse.widget-view+json":{"widget_id":"3e0a531f-a29a-46b1-822b-5587274855d0","widget_type":"Synapse.DataFrame"},"text/plain":"SynapseWidget(Synapse.DataFrame, 3e0a531f-a29a-46b1-822b-5587274855d0)"},"metadata":{}}],"execution_count":18,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"4908cce3-b5cc-4da0-97be-ef9215873c2a"},{"cell_type":"code","source":[" from pyspark.sql.functions import when, lit, col, current_timestamp, input_file_name\n","    \n"," # Add columns IsFlagged, CreatedTS and ModifiedTS\n"," df = df.withColumn(\"FileName\", input_file_name()) \\\n","     .withColumn(\"IsFlagged\", when(col(\"OrderDate\") < '2019-08-01',True).otherwise(False)) \\\n","     .withColumn(\"CreatedTS\", current_timestamp()).withColumn(\"ModifiedTS\", current_timestamp())\n","    \n"," # Update CustomerName to \"Unknown\" if CustomerName null or empty\n"," df = df.withColumn(\"CustomerName\", when((col(\"CustomerName\").isNull() | (col(\"CustomerName\")==\"\")),lit(\"Unknown\")).otherwise(col(\"CustomerName\")))\n"," "],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"11bbb665-bb13-40fa-823b-4daa08e24ec4","statement_id":21,"state":"finished","livy_statement_state":"available","queued_time":"2024-04-06T20:03:01.156347Z","session_start_time":null,"execution_start_time":"2024-04-06T20:03:05.2523076Z","execution_finish_time":"2024-04-06T20:03:05.4701943Z","parent_msg_id":"77603bf4-d4d6-47ea-9c1e-2af9c472d753"},"text/plain":"StatementMeta(, 11bbb665-bb13-40fa-823b-4daa08e24ec4, 21, Finished, Available)"},"metadata":{}}],"execution_count":19,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"988641df-ea78-4e9b-8e4c-3c768d3792c1"},{"cell_type":"code","source":[" # Define the schema for the sales_silver table\n","    \n"," from pyspark.sql.types import *\n"," from delta.tables import *\n","    \n"," DeltaTable.createIfNotExists(spark) \\\n","     .tableName(\"sales.sales_silver\") \\\n","     .addColumn(\"SalesOrderNumber\", StringType()) \\\n","     .addColumn(\"SalesOrderLineNumber\", IntegerType()) \\\n","     .addColumn(\"OrderDate\", DateType()) \\\n","     .addColumn(\"CustomerName\", StringType()) \\\n","     .addColumn(\"Email\", StringType()) \\\n","     .addColumn(\"Item\", StringType()) \\\n","     .addColumn(\"Quantity\", IntegerType()) \\\n","     .addColumn(\"UnitPrice\", FloatType()) \\\n","     .addColumn(\"Tax\", FloatType()) \\\n","     .addColumn(\"FileName\", StringType()) \\\n","     .addColumn(\"IsFlagged\", BooleanType()) \\\n","     .addColumn(\"CreatedTS\", DateType()) \\\n","     .addColumn(\"ModifiedTS\", DateType()) \\\n","     .execute()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"11bbb665-bb13-40fa-823b-4daa08e24ec4","statement_id":22,"state":"finished","livy_statement_state":"available","queued_time":"2024-04-06T20:03:01.2307298Z","session_start_time":null,"execution_start_time":"2024-04-06T20:03:05.8508222Z","execution_finish_time":"2024-04-06T20:03:08.1785085Z","parent_msg_id":"cc749bd3-861f-471d-ad9c-bfb3892acfa0"},"text/plain":"StatementMeta(, 11bbb665-bb13-40fa-823b-4daa08e24ec4, 22, Finished, Available)"},"metadata":{}},{"output_type":"execute_result","execution_count":65,"data":{"text/plain":"<delta.tables.DeltaTable at 0x70c35104b100>"},"metadata":{}}],"execution_count":20,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"7a8c3731-981c-4a07-942f-10388d0bc30e"},{"cell_type":"code","source":["data = spark.read.format(\"delta\").load(\"sales_silver\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"11bbb665-bb13-40fa-823b-4daa08e24ec4","statement_id":24,"state":"finished","livy_statement_state":"available","queued_time":"2024-04-06T20:07:35.1099117Z","session_start_time":null,"execution_start_time":"2024-04-06T20:07:35.5482624Z","execution_finish_time":"2024-04-06T20:07:40.3495936Z","parent_msg_id":"9b434cf0-fd61-491d-87f6-fae3808b8942"},"text/plain":"StatementMeta(, 11bbb665-bb13-40fa-823b-4daa08e24ec4, 24, Finished, Available)"},"metadata":{}},{"output_type":"error","ename":"Py4JJavaError","evalue":"An error occurred while calling o6382.load.\n: java.util.concurrent.ExecutionException: Operation failed: \"Bad Request\", 400, HEAD, http://onelake.dfs.fabric.microsoft.com/b9d75216-2af9-4a18-afd8-ffaa66921828/45755413-25be-442e-af93-e6425362f7d6/sales_silver/_delta_log?upn=false&action=getStatus&timeout=90\n\tat com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306)\n\tat com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293)\n\tat com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)\n\tat com.google.common.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)\n\tat com.google.common.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2410)\n\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2380)\n\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\n\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)\n\tat com.google.common.cache.LocalCache.get(LocalCache.java:4000)\n\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)\n\tat org.apache.spark.sql.delta.DeltaLog$.getDeltaLogFromCache$1(DeltaLog.scala:820)\n\tat org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:830)\n\tat org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:677)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.liftedTree1$1(DeltaTableV2.scala:92)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:91)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:86)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$snapshot$3(DeltaTableV2.scala:144)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot$lzycompute(DeltaTableV2.scala:144)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot(DeltaTableV2.scala:125)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.toBaseRelation(DeltaTableV2.scala:200)\n\tat org.apache.spark.sql.delta.sources.DeltaDataSource.$anonfun$createRelation$5(DeltaDataSource.scala:230)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:141)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:139)\n\tat org.apache.spark.sql.delta.sources.DeltaDataSource.recordFrameProfile(DeltaDataSource.scala:49)\n\tat org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:188)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:236)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:219)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:219)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:188)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: Operation failed: \"Bad Request\", 400, HEAD, http://onelake.dfs.fabric.microsoft.com/b9d75216-2af9-4a18-afd8-ffaa66921828/45755413-25be-442e-af93-e6425362f7d6/sales_silver/_delta_log?upn=false&action=getStatus&timeout=90\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.completeExecute(AbfsRestOperation.java:231)\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.lambda$execute$0(AbfsRestOperation.java:191)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDurationOfInvocation(IOStatisticsBinding.java:464)\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:189)\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsClient.getPathStatus(AbfsClient.java:690)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getFileStatus(AzureBlobFileSystemStore.java:1046)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.getFileStatus(AzureBlobFileSystem.java:645)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.getFileStatus(AzureBlobFileSystem.java:635)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1760)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.exists(AzureBlobFileSystem.java:1231)\n\tat org.apache.spark.sql.delta.storage.HadoopFileSystemLogStore.listFrom(HadoopFileSystemLogStore.scala:87)\n\tat org.apache.spark.sql.delta.Checkpoints.findLastCompleteCheckpointBefore(Checkpoints.scala:527)\n\tat org.apache.spark.sql.delta.Checkpoints.findLastCompleteCheckpointBefore$(Checkpoints.scala:515)\n\tat org.apache.spark.sql.delta.DeltaLog.findLastCompleteCheckpointBefore(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:482)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:141)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:139)\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:134)\n\tat com.microsoft.spark.telemetry.delta.SynapseLoggingShim.recordOperation(SynapseLoggingShim.scala:111)\n\tat com.microsoft.spark.telemetry.delta.SynapseLoggingShim.recordOperation$(SynapseLoggingShim.scala:93)\n\tat org.apache.spark.sql.delta.DeltaLog.recordOperation(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:133)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:123)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:113)\n\tat org.apache.spark.sql.delta.DeltaLog.recordDeltaOperation(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:459)\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:470)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:141)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:139)\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:134)\n\tat com.microsoft.spark.telemetry.delta.SynapseLoggingShim.recordOperation(SynapseLoggingShim.scala:111)\n\tat com.microsoft.spark.telemetry.delta.SynapseLoggingShim.recordOperation$(SynapseLoggingShim.scala:93)\n\tat org.apache.spark.sql.delta.DeltaLog.recordOperation(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:133)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:123)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:113)\n\tat org.apache.spark.sql.delta.DeltaLog.recordDeltaOperation(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:459)\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:470)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:141)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:139)\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:134)\n\tat com.microsoft.spark.telemetry.delta.SynapseLoggingShim.recordOperation(SynapseLoggingShim.scala:111)\n\tat com.microsoft.spark.telemetry.delta.SynapseLoggingShim.recordOperation$(SynapseLoggingShim.scala:93)\n\tat org.apache.spark.sql.delta.DeltaLog.recordOperation(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:133)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:123)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:113)\n\tat org.apache.spark.sql.delta.DeltaLog.recordDeltaOperation(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:459)\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:470)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:141)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:139)\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:134)\n\tat com.microsoft.spark.telemetry.delta.SynapseLoggingShim.recordOperation(SynapseLoggingShim.scala:111)\n\tat com.microsoft.spark.telemetry.delta.SynapseLoggingShim.recordOperation$(SynapseLoggingShim.scala:93)\n\tat org.apache.spark.sql.delta.DeltaLog.recordOperation(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:133)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:123)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:113)\n\tat org.apache.spark.sql.delta.DeltaLog.recordDeltaOperation(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:459)\n\tat org.apache.spark.sql.delta.Checkpoints.readLastCheckpointFile(Checkpoints.scala:453)\n\tat org.apache.spark.sql.delta.Checkpoints.readLastCheckpointFile$(Checkpoints.scala:452)\n\tat org.apache.spark.sql.delta.DeltaLog.readLastCheckpointFile(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotAtInit$1(SnapshotManagement.scala:290)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:141)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:139)\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit(SnapshotManagement.scala:288)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit$(SnapshotManagement.scala:287)\n\tat org.apache.spark.sql.delta.DeltaLog.getSnapshotAtInit(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.$init$(SnapshotManagement.scala:57)\n\tat org.apache.spark.sql.delta.DeltaLog.<init>(DeltaLog.scala:80)\n\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$4(DeltaLog.scala:809)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$3(DeltaLog.scala:804)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:141)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:139)\n\tat org.apache.spark.sql.delta.DeltaLog$.recordFrameProfile(DeltaLog.scala:614)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:134)\n\tat com.microsoft.spark.telemetry.delta.SynapseLoggingShim.recordOperation(SynapseLoggingShim.scala:111)\n\tat com.microsoft.spark.telemetry.delta.SynapseLoggingShim.recordOperation$(SynapseLoggingShim.scala:93)\n\tat org.apache.spark.sql.delta.DeltaLog$.recordOperation(DeltaLog.scala:614)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:133)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:123)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:113)\n\tat org.apache.spark.sql.delta.DeltaLog$.recordDeltaOperation(DeltaLog.scala:614)\n\tat org.apache.spark.sql.delta.DeltaLog$.createDeltaLog$1(DeltaLog.scala:803)\n\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$5(DeltaLog.scala:821)\n\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4792)\n\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\n\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\n\t... 37 more\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","Cell \u001b[0;32mIn[71], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdelta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msales_silver\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py:300\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 300\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n","File \u001b[0;32m~/cluster-env/trident_env/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n","File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n","File \u001b[0;32m~/cluster-env/trident_env/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o6382.load.\n: java.util.concurrent.ExecutionException: Operation failed: \"Bad Request\", 400, HEAD, http://onelake.dfs.fabric.microsoft.com/b9d75216-2af9-4a18-afd8-ffaa66921828/45755413-25be-442e-af93-e6425362f7d6/sales_silver/_delta_log?upn=false&action=getStatus&timeout=90\n\tat com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306)\n\tat com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293)\n\tat com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)\n\tat com.google.common.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)\n\tat com.google.common.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2410)\n\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2380)\n\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\n\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)\n\tat com.google.common.cache.LocalCache.get(LocalCache.java:4000)\n\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)\n\tat org.apache.spark.sql.delta.DeltaLog$.getDeltaLogFromCache$1(DeltaLog.scala:820)\n\tat org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:830)\n\tat org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:677)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.liftedTree1$1(DeltaTableV2.scala:92)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:91)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:86)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$snapshot$3(DeltaTableV2.scala:144)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot$lzycompute(DeltaTableV2.scala:144)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot(DeltaTableV2.scala:125)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.toBaseRelation(DeltaTableV2.scala:200)\n\tat org.apache.spark.sql.delta.sources.DeltaDataSource.$anonfun$createRelation$5(DeltaDataSource.scala:230)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:141)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:139)\n\tat org.apache.spark.sql.delta.sources.DeltaDataSource.recordFrameProfile(DeltaDataSource.scala:49)\n\tat org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:188)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:236)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:219)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:219)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:188)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: Operation failed: \"Bad Request\", 400, HEAD, http://onelake.dfs.fabric.microsoft.com/b9d75216-2af9-4a18-afd8-ffaa66921828/45755413-25be-442e-af93-e6425362f7d6/sales_silver/_delta_log?upn=false&action=getStatus&timeout=90\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.completeExecute(AbfsRestOperation.java:231)\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.lambda$execute$0(AbfsRestOperation.java:191)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDurationOfInvocation(IOStatisticsBinding.java:464)\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:189)\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsClient.getPathStatus(AbfsClient.java:690)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getFileStatus(AzureBlobFileSystemStore.java:1046)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.getFileStatus(AzureBlobFileSystem.java:645)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.getFileStatus(AzureBlobFileSystem.java:635)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1760)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.exists(AzureBlobFileSystem.java:1231)\n\tat org.apache.spark.sql.delta.storage.HadoopFileSystemLogStore.listFrom(HadoopFileSystemLogStore.scala:87)\n\tat org.apache.spark.sql.delta.Checkpoints.findLastCompleteCheckpointBefore(Checkpoints.scala:527)\n\tat org.apache.spark.sql.delta.Checkpoints.findLastCompleteCheckpointBefore$(Checkpoints.scala:515)\n\tat org.apache.spark.sql.delta.DeltaLog.findLastCompleteCheckpointBefore(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:482)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:141)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:139)\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:134)\n\tat com.microsoft.spark.telemetry.delta.SynapseLoggingShim.recordOperation(SynapseLoggingShim.scala:111)\n\tat com.microsoft.spark.telemetry.delta.SynapseLoggingShim.recordOperation$(SynapseLoggingShim.scala:93)\n\tat org.apache.spark.sql.delta.DeltaLog.recordOperation(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:133)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:123)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:113)\n\tat org.apache.spark.sql.delta.DeltaLog.recordDeltaOperation(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:459)\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:470)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:141)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:139)\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:134)\n\tat com.microsoft.spark.telemetry.delta.SynapseLoggingShim.recordOperation(SynapseLoggingShim.scala:111)\n\tat com.microsoft.spark.telemetry.delta.SynapseLoggingShim.recordOperation$(SynapseLoggingShim.scala:93)\n\tat org.apache.spark.sql.delta.DeltaLog.recordOperation(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:133)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:123)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:113)\n\tat org.apache.spark.sql.delta.DeltaLog.recordDeltaOperation(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:459)\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:470)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:141)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:139)\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:134)\n\tat com.microsoft.spark.telemetry.delta.SynapseLoggingShim.recordOperation(SynapseLoggingShim.scala:111)\n\tat com.microsoft.spark.telemetry.delta.SynapseLoggingShim.recordOperation$(SynapseLoggingShim.scala:93)\n\tat org.apache.spark.sql.delta.DeltaLog.recordOperation(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:133)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:123)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:113)\n\tat org.apache.spark.sql.delta.DeltaLog.recordDeltaOperation(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:459)\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:470)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:141)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:139)\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:134)\n\tat com.microsoft.spark.telemetry.delta.SynapseLoggingShim.recordOperation(SynapseLoggingShim.scala:111)\n\tat com.microsoft.spark.telemetry.delta.SynapseLoggingShim.recordOperation$(SynapseLoggingShim.scala:93)\n\tat org.apache.spark.sql.delta.DeltaLog.recordOperation(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:133)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:123)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:113)\n\tat org.apache.spark.sql.delta.DeltaLog.recordDeltaOperation(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:459)\n\tat org.apache.spark.sql.delta.Checkpoints.readLastCheckpointFile(Checkpoints.scala:453)\n\tat org.apache.spark.sql.delta.Checkpoints.readLastCheckpointFile$(Checkpoints.scala:452)\n\tat org.apache.spark.sql.delta.DeltaLog.readLastCheckpointFile(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotAtInit$1(SnapshotManagement.scala:290)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:141)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:139)\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit(SnapshotManagement.scala:288)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit$(SnapshotManagement.scala:287)\n\tat org.apache.spark.sql.delta.DeltaLog.getSnapshotAtInit(DeltaLog.scala:74)\n\tat org.apache.spark.sql.delta.SnapshotManagement.$init$(SnapshotManagement.scala:57)\n\tat org.apache.spark.sql.delta.DeltaLog.<init>(DeltaLog.scala:80)\n\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$4(DeltaLog.scala:809)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$3(DeltaLog.scala:804)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:141)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:139)\n\tat org.apache.spark.sql.delta.DeltaLog$.recordFrameProfile(DeltaLog.scala:614)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:134)\n\tat com.microsoft.spark.telemetry.delta.SynapseLoggingShim.recordOperation(SynapseLoggingShim.scala:111)\n\tat com.microsoft.spark.telemetry.delta.SynapseLoggingShim.recordOperation$(SynapseLoggingShim.scala:93)\n\tat org.apache.spark.sql.delta.DeltaLog$.recordOperation(DeltaLog.scala:614)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:133)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:123)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:113)\n\tat org.apache.spark.sql.delta.DeltaLog$.recordDeltaOperation(DeltaLog.scala:614)\n\tat org.apache.spark.sql.delta.DeltaLog$.createDeltaLog$1(DeltaLog.scala:803)\n\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$5(DeltaLog.scala:821)\n\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4792)\n\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\n\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\n\t... 37 more\n"]}],"execution_count":22,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"advisor":{"adviceMetadata":"{\"artifactId\":\"24d3596f-f543-4cb1-8212-852ec971aff8\",\"activityId\":\"11bbb665-bb13-40fa-823b-4daa08e24ec4\",\"applicationId\":\"application_1712431678541_0001\",\"jobGroupId\":\"24\",\"advices\":{\"error\":1}}"}},"id":"fcb44a99-3ded-48a2-b2ea-9302f5a7206b"},{"cell_type":"code","source":["# Update existing records and insert new ones based on a condition defined by the columns SalesOrderNumber, OrderDate, CustomerName, and Item.\n","\n","from delta.tables import *\n","    \n","deltaTable = DeltaTable.forPath(spark, 'abfss://HCH_Fabric_Playground_Test@onelake.dfs.fabric.microsoft.com/Sales.Lakehouse/Tables/sales_silver')\n","\n","dfUpdates = df\n","    \n","deltaTable.alias('silver') \\\n","  .merge(\n","    dfUpdates.alias('updates'),\n","    'silver.SalesOrderNumber = updates.SalesOrderNumber and silver.OrderDate = updates.OrderDate and silver.CustomerName = updates.CustomerName and silver.Item = updates.Item'\n","  ) \\\n","   .whenMatchedUpdate(set =\n","    {\n","          \n","    }\n","  ) \\\n"," .whenNotMatchedInsert(values =\n","    {\n","      \"SalesOrderNumber\": \"updates.SalesOrderNumber\",\n","      \"SalesOrderLineNumber\": \"updates.SalesOrderLineNumber\",\n","      \"OrderDate\": \"updates.OrderDate\",\n","      \"CustomerName\": \"updates.CustomerName\",\n","      \"Email\": \"updates.Email\",\n","      \"Item\": \"updates.Item\",\n","      \"Quantity\": \"updates.Quantity\",\n","      \"UnitPrice\": \"updates.UnitPrice\",\n","      \"Tax\": \"updates.Tax\",\n","      \"FileName\": \"updates.FileName\",\n","      \"IsFlagged\": \"updates.IsFlagged\",\n","      \"CreatedTS\": \"updates.CreatedTS\",\n","      \"ModifiedTS\": \"updates.ModifiedTS\"\n","    }\n","  ) \\\n","  .execute()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"11bbb665-bb13-40fa-823b-4daa08e24ec4","statement_id":29,"state":"finished","livy_statement_state":"available","queued_time":"2024-04-06T20:10:32.6519528Z","session_start_time":null,"execution_start_time":"2024-04-06T20:10:32.9983294Z","execution_finish_time":"2024-04-06T20:10:42.8383177Z","parent_msg_id":"f58db3ef-3772-465f-b89b-1e1559d785a2"},"text/plain":"StatementMeta(, 11bbb665-bb13-40fa-823b-4daa08e24ec4, 29, Finished, Available)"},"metadata":{}}],"execution_count":27,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"6f2aa390-2fe9-46d1-a6bd-c1dc2b46a1e5"},{"cell_type":"code","source":[],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"19dfacf7-f64a-4750-bf50-641fd0f94e89"}],"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"synapse_widget":{"version":"0.1","state":{"3e0a531f-a29a-46b1-822b-5587274855d0":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"SO49172","1":"1","2":"2021-01-01","3":"Brian Howard","4":"brian23@adventure-works.com","5":"Road-250 Red, 44","6":"1","7":"2443.35009765625","8":"195.46800231933594","index":1},{"0":"SO49173","1":"1","2":"2021-01-01","3":"Linda Alvarez","4":"linda19@adventure-works.com","5":"Mountain-200 Silver, 38","6":"1","7":"2071.419677734375","8":"165.71359252929688","index":2},{"0":"SO49174","1":"1","2":"2021-01-01","3":"Gina Hernandez","4":"gina4@adventure-works.com","5":"Mountain-200 Silver, 42","6":"1","7":"2071.419677734375","8":"165.71359252929688","index":3},{"0":"SO49178","1":"1","2":"2021-01-01","3":"Beth Ruiz","4":"beth4@adventure-works.com","5":"Road-550-W Yellow, 44","6":"1","7":"1000.4375","8":"80.03500366210938","index":4},{"0":"SO49179","1":"1","2":"2021-01-01","3":"Evan Ward","4":"evan13@adventure-works.com","5":"Road-550-W Yellow, 38","6":"1","7":"1000.4375","8":"80.03500366210938","index":5},{"0":"SO49175","1":"1","2":"2021-01-01","3":"Margaret Guo","4":"margaret24@adventure-works.com","5":"Road-250 Red, 52","6":"1","7":"2443.35009765625","8":"195.46800231933594","index":6},{"0":"SO49180","1":"1","2":"2021-01-01","3":"Mitchell Yuan","4":"mitchell6@adventure-works.com","5":"Road-650 Black, 58","6":"1","7":"782.989990234375","8":"62.639198303222656","index":7},{"0":"SO49176","1":"1","2":"2021-01-01","3":"Shawn Sharma","4":"shawn11@adventure-works.com","5":"Mountain-200 Silver, 46","6":"1","7":"2071.419677734375","8":"165.71359252929688","index":8},{"0":"SO49177","1":"1","2":"2021-01-01","3":"Barbara Chande","4":"barbara44@adventure-works.com","5":"Mountain-200 Silver, 42","6":"1","7":"2071.419677734375","8":"165.71359252929688","index":9},{"0":"SO49186","1":"1","2":"2021-01-02","3":"Cara Xu","4":"cara8@adventure-works.com","5":"Road-250 Red, 52","6":"1","7":"2443.35009765625","8":"195.46800231933594","index":10}],"schema":[{"key":"0","name":"SalesOrderNumber","type":"string"},{"key":"1","name":"SalesOrderLineNumber","type":"bigint"},{"key":"2","name":"OrderDate","type":"date"},{"key":"3","name":"CustomerName","type":"string"},{"key":"4","name":"Email","type":"string"},{"key":"5","name":"Item","type":"string"},{"key":"6","name":"Quantity","type":"bigint"},{"key":"7","name":"UnitPrice","type":"double"},{"key":"8","name":"Tax","type":"double"}],"truncated":false},"isSummary":false,"language":"scala"},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["0"],"seriesFieldKeys":["0"],"aggregationType":"count","isStacked":false,"binsNumber":10,"wordFrequency":"-1"}}}}}},"dependencies":{"lakehouse":{"default_lakehouse":"45755413-25be-442e-af93-e6425362f7d6","known_lakehouses":[{"id":"45755413-25be-442e-af93-e6425362f7d6"},{"id":"4329d74c-56c2-4021-b112-7c9046567172"}]}}},"nbformat":4,"nbformat_minor":5}